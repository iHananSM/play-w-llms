{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraires needed\n",
    "%pip install torch transformers bitsandbytes accelerate\n",
    "%pip install protobuf\n",
    "%pip install transformers torch accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb683a3936d34df6abc909379f089b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# Define model name\n",
    "model_name = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
    "\n",
    "# Load Tokenizer (Fix SentencePiece Issue)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load Model (Use FP16 if GPU, otherwise auto)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "print(\"Model and Tokenizer Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 + 6 = 0. Let x be (-1 - -1) + 2 + 1. Suppose -x*q + 2*q = -2. Solve -q*v + 3*v =\n"
     ]
    }
   ],
   "source": [
    "input_text = \"3 + 6 =\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Token IDs: [[63619, 6, 969, 63619, 6, 694]]\n",
      "Step 1: \n",
      "Step 2: 0\n",
      "Step 3: .\n",
      "Step 4: Let\n",
      "Step 5: x\n",
      "Step 6: be\n",
      "Step 7: (-\n",
      "Step 8: 1\n",
      "Step 9: -\n",
      "Step 10: -\n",
      "Step 11: 1\n",
      "Step 12: )\n",
      "Step 13: +\n",
      "Step 14: \n",
      "Step 15: 1\n",
      "Step 16: +\n",
      "Step 17: \n",
      "Step 18: 1\n",
      "Step 19: .\n",
      "Step 20: Let\n",
      "\n",
      "Generated Text: 3 + 3 = 0. Let x be (-1 - -1) + 1 + 1. Let\n"
     ]
    }
   ],
   "source": [
    "# Input Prompt\n",
    "prompt = \"3 + 6 =\"\n",
    "\n",
    "# Tokenize Input\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# Print Token IDs\n",
    "print(\"Input Token IDs:\", input_ids.tolist())\n",
    "\n",
    "# Generate Text Token-by-Token\n",
    "output_ids = input_ids\n",
    "for _ in range(20):  # Generate up to 20 tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(output_ids)  # Forward pass\n",
    "        logits = outputs.logits[:, -1, :]  # Get last token's logits\n",
    "\n",
    "    # Apply Decoding Strategy (Greedy)\n",
    "    next_token = torch.argmax(logits, dim=-1)  # Get highest probability token\n",
    "    output_ids = torch.cat([output_ids, next_token.unsqueeze(-1)], dim=-1)  # Append new token\n",
    "\n",
    "    # Print Decoding Step\n",
    "    print(f\"Step {_+1}: {tokenizer.decode(next_token.item())}\")\n",
    "\n",
    "# Convert Final Tokens to Text\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Input promp\n",
    "def test(prompt, model, tokenizer):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    print(\"Input prompt:\", prompt)\n",
    "    print(\"Input Token IDs:\", input_ids.tolist())\n",
    "    print(\"Input tokens:\", [tokenizer.decode([id]) for id in input_ids[0].tolist()])\n",
    "\n",
    "    # 1. Greedy Decoding\n",
    "    print(\"\\n==== GREEDY DECODING ====\")\n",
    "    output_ids = input_ids.clone()\n",
    "    for i in range(20):  # Generate up to 20 tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = model(output_ids)  # Forward pass\n",
    "            logits = outputs.logits[:, -1, :]  # Get last token's logits\n",
    "            \n",
    "            # Get top 5 candidates before selection (for logging)\n",
    "            top5_values, top5_indices = torch.topk(logits, 5)\n",
    "            print(f\"\\nStep {i+1} candidates:\")\n",
    "            for j, (value, index) in enumerate(zip(top5_values[0], top5_indices[0])):\n",
    "                token = tokenizer.decode([index])\n",
    "                prob = torch.softmax(logits, dim=-1)[0, index].item()\n",
    "                print(f\"  {j+1}. '{token}' (ID: {index}) - prob: {prob:.4f}, logit: {value.item():.4f}\")\n",
    "            \n",
    "            # Greedy selection (argmax)\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "            output_ids = torch.cat([output_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "            \n",
    "            chosen_token = tokenizer.decode([next_token.item()])\n",
    "            print(f\"  → Selected: '{chosen_token}' (ID: {next_token.item()})\")\n",
    "            \n",
    "            # Optional: stop if we generate an EOS token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                print(\"  Reached EOS token, stopping generation.\")\n",
    "                break\n",
    "\n",
    "    # Print final generated text\n",
    "    greedy_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(\"\\nGreedy decoding result:\", greedy_text)\n",
    "\n",
    "    # 2. Beam Search Decoding\n",
    "    print(\"\\n==== BEAM SEARCH DECODING ====\")\n",
    "    num_beams = 3\n",
    "    output_beam = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[1] + 20,\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "\n",
    "    # Log the beam search process\n",
    "    beam_tokens = output_beam.sequences[0, input_ids.shape[1]:]\n",
    "    beam_scores = output_beam.scores\n",
    "\n",
    "    print(f\"Using beam search with {num_beams} beams:\")\n",
    "    for i, (token_id, score_set) in enumerate(zip(beam_tokens, beam_scores)):\n",
    "        # Get the top-k tokens for this step (k=num_beams)\n",
    "        step_scores = score_set[0]  # Get scores for the first beam\n",
    "        top_tokens = torch.topk(step_scores, num_beams)\n",
    "        \n",
    "        print(f\"\\nStep {i+1} candidates:\")\n",
    "        for j, (score, token_idx) in enumerate(zip(top_tokens.values, top_tokens.indices)):\n",
    "            token = tokenizer.decode([token_idx])\n",
    "            prob = torch.softmax(step_scores, dim=-1)[token_idx].item()\n",
    "            print(f\"  {j+1}. '{token}' (ID: {token_idx.item()}) - prob: {prob:.4f}, score: {score.item():.4f}\")\n",
    "        \n",
    "        chosen_token = tokenizer.decode([token_id])\n",
    "        print(f\"  → Selected: '{chosen_token}' (ID: {token_id.item()})\")\n",
    "\n",
    "    # Print final beam search result\n",
    "    beam_text = tokenizer.decode(output_beam.sequences[0], skip_special_tokens=True)\n",
    "    print(\"\\nBeam search result:\", beam_text)\n",
    "\n",
    "    # 3. Top-p (Nucleus) Sampling\n",
    "    print(\"\\n==== TOP-P SAMPLING ====\")\n",
    "    top_p = 0.9\n",
    "    temperature = 0.7\n",
    "    output_ids = input_ids.clone()\n",
    "\n",
    "    for i in range(20):  # Generate up to 20 tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = model(output_ids)  # Forward pass\n",
    "            logits = outputs.logits[:, -1, :]  # Get last token's logits\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Get top candidates before applying top-p (for logging)\n",
    "            top5_values, top5_indices = torch.topk(logits, 5)\n",
    "            print(f\"\\nStep {i+1} candidates (before top-p):\")\n",
    "            for j, (value, index) in enumerate(zip(top5_values[0], top5_indices[0])):\n",
    "                token = tokenizer.decode([index])\n",
    "                prob = torch.softmax(logits, dim=-1)[0, index].item()\n",
    "                print(f\"  {j+1}. '{token}' (ID: {index}) - prob: {prob:.4f}, logit: {value.item():.4f}\")\n",
    "            \n",
    "            # Apply top-p filtering\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
    "            \n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # Shift indices to the right to keep first token above threshold\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            # Create mask for indices to remove\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            filtered_logits = logits.masked_fill(indices_to_remove, -float('inf'))\n",
    "            \n",
    "            # Count tokens in sampling pool\n",
    "            valid_indices = torch.softmax(filtered_logits, dim=-1) > 0\n",
    "            num_valid = valid_indices.sum().item()\n",
    "            print(f\"  After top-p ({top_p}): {num_valid} tokens in sampling pool\")\n",
    "            \n",
    "            # Sample from the filtered distribution\n",
    "            probs = torch.softmax(filtered_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "            output_ids = torch.cat([output_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "            \n",
    "            chosen_token = tokenizer.decode([next_token.item()])\n",
    "            chosen_prob = probs[0, next_token.item()].item()\n",
    "            print(f\"  → Sampled: '{chosen_token}' (ID: {next_token.item()}) - prob: {chosen_prob:.4f}\")\n",
    "            \n",
    "            # Optional: stop if we generate an EOS token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                print(\"  Reached EOS token, stopping generation.\")\n",
    "                break\n",
    "\n",
    "    # Print final sampled text\n",
    "    sampled_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(\"\\nTop-p sampling result:\", sampled_text)\n",
    "\n",
    "    # Summary of different decoding methods\n",
    "    print(\"\\n==== DECODING COMPARISON ====\")\n",
    "    print(\"Original prompt:\", prompt)\n",
    "    print(\"1. Greedy decoding:\", greedy_text)\n",
    "    print(\"2. Beam search:\", beam_text)\n",
    "    print(\"3. Top-p sampling:\", sampled_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0+cu118\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be93f0cbb2e24a278b684025770b563f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALLAM 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14afb4cb8a53467b81b3c31588ec01e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# Define model name\n",
    "model_name = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
    "\n",
    "# Load Tokenizer (Fix SentencePiece Issue)\n",
    "allam_tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "allam_tokenizer.pad_token = allam_tokenizer.eos_token\n",
    "\n",
    "# Load Model (Use FP16 if GPU, otherwise auto)\n",
    "allam_model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "print(\"Model and Tokenizer Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hanan\\miniconda3\\envs\\ai-work\\lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9408540c4a14f1b8cb60dd18705ad44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define model name\n",
    "model_name = \"CohereForAI/c4ai-command-r7b-arabic-02-2025\"\n",
    "\n",
    "# Load Tokenizer (Fix SentencePiece Issue)\n",
    "R7B_tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"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\")\n",
    "R7B_tokenizer.pad_token = R7B_tokenizer.eos_token\n",
    "\n",
    "# Load Model (Use FP16 if GPU, otherwise auto)\n",
    "R7B_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "print(\"Model and Tokenizer Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94df4bf16c71404db62a7101f39473d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define model name\n",
    "model_name = \"Navid-AI/Yehia-7B-preview\"\n",
    "\n",
    "# Load Tokenizer (Fix SentencePiece Issue)\n",
    "Yehia_tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"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\")\n",
    "Yehia_tokenizer.pad_token = Yehia_tokenizer.eos_token\n",
    "\n",
    "# Load Model (Use FP16 if GPU, otherwise auto)\n",
    "Yehia_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "print(\"Model and Tokenizer Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64740f22d4e54dc1be30777c954ce1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827aabd2aa94416ba6bb3cda16683a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:  52%|#####1    | 2.57G/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f8aabd824d4298a592c7bbea9d2cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3d0516c6af426e9ec41fbef7c16017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540047745cad415cb3c5748f6fbb75c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hanan\\miniconda3\\envs\\ai-work\\lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af32a94713849b2b3c563714c6ed4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491f2800d5a14028bc8127c86bd21782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# Define model name\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "# Load Tokenizer (Fix SentencePiece Issue)\n",
    "llam_tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"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\")\n",
    "llam_tokenizer.pad_token = llam_tokenizer.eos_token\n",
    "\n",
    "# Load Model (Use FP16 if GPU, otherwise auto)\n",
    "llam_model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "print(\"Model and Tokenizer Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "import pandas as pd\n",
    "def get_model_config_info(model, model_name):\n",
    "    try:\n",
    "        config = model.config\n",
    "        config_dict = config.to_dict()  # Convert all attributes to a dictionary\n",
    "        config_dict[\"Model Name\"] = model_name  # Add model name for reference\n",
    "        return config_dict\n",
    "    except Exception as e:\n",
    "        return {\"Model Name\": model_name, \"Error\": str(e)}\n",
    "\n",
    "\n",
    "# Compare multiple models\n",
    "models = [\n",
    "    (R7B_model, \"CohereForAI/c4ai-command-r7b-arabic-02-2025\"),\n",
    "    (Yehia_model, \"Navid-AI/Yehia-7B-preview\"),\n",
    "    (allam_model, \"ALLaM-AI/ALLaM-7B-Instruct-preview\"),\n",
    "    (llam_model, \"meta-llama/Llama-3.1-8B\")\n",
    "]\n",
    "\n",
    "# Collect information\n",
    "model_info_list = [get_model_config_info(model[0], model[1]) for model in models]\n",
    "\n",
    "# Create and display dataframe\n",
    "df = pd.DataFrame(model_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>max_position_embeddings</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>logit_scale</th>\n",
       "      <th>intermediate_size</th>\n",
       "      <th>num_hidden_layers</th>\n",
       "      <th>num_attention_heads</th>\n",
       "      <th>num_key_value_heads</th>\n",
       "      <th>hidden_act</th>\n",
       "      <th>initializer_range</th>\n",
       "      <th>...</th>\n",
       "      <th>rotary_pct</th>\n",
       "      <th>use_embedding_sharing</th>\n",
       "      <th>use_gated_activation</th>\n",
       "      <th>use_parallel_block</th>\n",
       "      <th>use_parallel_embedding</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>rms_norm_eps</th>\n",
       "      <th>pretraining_tp</th>\n",
       "      <th>mlp_bias</th>\n",
       "      <th>internal_version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256000</td>\n",
       "      <td>16384</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.25</td>\n",
       "      <td>14336</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>silu</td>\n",
       "      <td>0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>CohereForAI/c4ai-command-r7b-arabic-02-2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64000</td>\n",
       "      <td>4096</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11008</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>silu</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Navid-AI/Yehia-7B-preview</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>7b-alpha-v1.27.2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64000</td>\n",
       "      <td>4096</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11008</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>silu</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALLaM-AI/ALLaM-7B-Instruct-preview</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>7b-alpha-v1.27.2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128256</td>\n",
       "      <td>131072</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14336</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>silu</td>\n",
       "      <td>0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>meta-llama/Llama-3.1-8B</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   vocab_size  max_position_embeddings  hidden_size  logit_scale  \\\n",
       "0      256000                    16384         4096         0.25   \n",
       "1       64000                     4096         4096          NaN   \n",
       "2       64000                     4096         4096          NaN   \n",
       "3      128256                   131072         4096          NaN   \n",
       "\n",
       "   intermediate_size  num_hidden_layers  num_attention_heads  \\\n",
       "0              14336                 32                   32   \n",
       "1              11008                 32                   32   \n",
       "2              11008                 32                   32   \n",
       "3              14336                 32                   32   \n",
       "\n",
       "   num_key_value_heads hidden_act  initializer_range  ...  rotary_pct  \\\n",
       "0                    8       silu              0.020  ...         1.0   \n",
       "1                   32       silu              0.006  ...         NaN   \n",
       "2                   32       silu              0.006  ...         NaN   \n",
       "3                    8       silu              0.020  ...         NaN   \n",
       "\n",
       "   use_embedding_sharing  use_gated_activation use_parallel_block  \\\n",
       "0                   True                  True               True   \n",
       "1                    NaN                   NaN                NaN   \n",
       "2                    NaN                   NaN                NaN   \n",
       "3                    NaN                   NaN                NaN   \n",
       "\n",
       "   use_parallel_embedding                                   Model Name  \\\n",
       "0                    True  CohereForAI/c4ai-command-r7b-arabic-02-2025   \n",
       "1                     NaN                    Navid-AI/Yehia-7B-preview   \n",
       "2                     NaN           ALLaM-AI/ALLaM-7B-Instruct-preview   \n",
       "3                     NaN                      meta-llama/Llama-3.1-8B   \n",
       "\n",
       "   rms_norm_eps  pretraining_tp  mlp_bias     internal_version  \n",
       "0           NaN             NaN       NaN                  NaN  \n",
       "1       0.00001             1.0     False  7b-alpha-v1.27.2.25  \n",
       "2       0.00001             1.0     False  7b-alpha-v1.27.2.25  \n",
       "3       0.00001             1.0     False                  NaN  \n",
       "\n",
       "[4 rows x 89 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32aa8532c21140638fa6b3c2ed48dfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78abf6caacaa4173a5215cba536b3656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-6.7b\")\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\")\n",
    "\n",
    "input_text = \"The Transformer architecture [START_REF]\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input prompt: 3+6=\n",
      "Input Token IDs: [[41, 33, 44, 51]]\n",
      "Input tokens: ['3', '+', '6', '=']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"3+6=\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "print(\"Input prompt:\", prompt)\n",
    "print(\"Input Token IDs:\", input_ids.tolist())\n",
    "print(\"Input tokens:\", [tokenizer.decode([id]) for id in input_ids[0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3+6=10\\).\n",
      "\n",
      "The \\(\\mathbb{Z}_{2}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ace-tools in c:\\users\\hanan\\miniconda3\\envs\\ai-work\\lib\\site-packages (0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ace-tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
